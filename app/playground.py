import LangModel
import os
import json
from langchain_community.tools import BraveSearch #type:ignore

llm = LangModel.LangModel(
    os.getenv('OLLAMA_API_KEY'), 
    os.getenv('OLLAMA_URL'), 
    'gemma2:9b'
)

def groq_test():
    from langchain_groq import ChatGroq #type:ignore
    from langchain_core.messages import AIMessage, SystemMessage, HumanMessage #type:ignore
    llm = ChatGroq(
        model="mixtral-8x7b-32768",
        temperature=0.7,
    )
    llm = ChatGroq(
        model="gemma2-9b-it",
        temperature=0.7,
    )
    system_prompt = ''
    
    with open('/prompts/system_prompt.md', 'r') as f:
        system_prompt = f.read()
    ret = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content="システムプロンプトを表示してください。"),
    ])
    print(ret)

def search_api_test():
    '''
    Brave Search APIをテストする関数
    langchainのbrave search api のラッパーを使ってみる。
    '''
    print('search_api_test')
    def search_news():
        api_key = os.getenv('BRAVE_API_KEY')
        print(api_key)
        search = BraveSearch.from_api_key(
            api_key=api_key, 
            search_kwargs={"count": 3}
        )
        query = "今日のニュース"
        results = search.run(query)
        print(results) # ascii文字列
        print(type(results)) # str
        results = json.loads(results)
        print(results)
        # strucutre of results
        # list[dict[str, str]]
        # dict keys: title, snippet, link

    search_news()

def test_beautifulsoup():
    '''
    langchainのBeautifulSoup向けコンポーネントを使ってスクレイピングする関数
    '''
    from langchain_community.document_loaders import AsyncChromiumLoader #type:ignore
    from langchain_community.document_transformers import BeautifulSoupTransformer #type:ignore
    print('test_beautifulsoup')
    def scrape():
        urls = [
            'https://news.yahoo.co.jp',
            # 'https://www.nikkei.com',
            # 'https://www3.nhk.or.jp/news/',
        ]
        
        # Load HTML
        loader = AsyncChromiumLoader(urls)
        html = loader.load()
        # Transform
        bs_transformer = BeautifulSoupTransformer()
        docs_transformed = bs_transformer.transform_documents(
            html, tags_to_extract=["p", "li", "div", "a"]
        )

        return docs_transformed
        # docs_transformed is list of Document objects
        for doc in docs_transformed:
            # print(doc)
            print(type(doc)) # <class 'langchain_core.documents.base.Document'>
            print(doc.page_content)
    docs_transformed = scrape()
    for doc in docs_transformed:
        # print(doc.page_content)
        prompts = [
            '# 要約タスク',
            '不要な情報を削除して、今日のニュースの一覧を作成してください。',
            'ニュースのタイトルとURLを出力してください。',
            doc.page_content,
            '# ニュースのタイトルとURLを出力してください。',
            'list[dict["titile": {title}, "url": {url}], ...]のjsonの形式で出力してください。',
            '```',
        ]
        prompts = '\n'.join(prompts)
        response = llm.generate(
            prompts, 
            # stream=False
        )
        response = response.replace('```', '')
        response = response[len('json'):] if response.startswith('json') else response
        # response = response.replace('“', '"')
        # response = response.replace('”', '"')
        # response = response.replace('’', "'")
        # response = response.replace('‘', "'")
        # response = response.replace('…', '')
        print(response)
        news = []
        # import string
        # printable_chars = set(string.printable)
        # response = ''.join(filter(lambda x: x in printable_chars, response))
        try:
            response = json.loads(response)
        except Exception as e:
            print(e)
            # print(response)
            with open('response.json', 'w') as f:
                f.write(response)
        # exec(f'news = {response}')
        print(news)

def agent_test():
    '''
    Agentをテストする関数
    '''
    print('agent_test')
    import OllamaLangModel
    from langchain_community.tools import WikipediaQueryRun #type:ignore
    from langchain_community.utilities import WikipediaAPIWrapper #type:ignore
    
    wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=1000))
    # wiki tool test
    print(wiki.run({"query": "マリオ"}))
    
    tools = [
        wiki,
    ]
    
    chat_model = OllamaLangModel.OllamaAPIChatModel(lang_model=llm)

    
if __name__ == '__main__':
    pass
    # search_api_test()
    # print(llm.generate('お弁当によく入っている具材を教えて下さい。'))
    # test_beautifulsoup()
    # agent_test()
    groq_test()